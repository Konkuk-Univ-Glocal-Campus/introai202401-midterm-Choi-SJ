#pip install torchvision
#pip install torchinfo
#pip install pytorchcv

import torch
import torchvision
from torchvision import transforms
import matplotlib.pyplot as plt
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# 데이터 전처리 과정 정의
transform = transforms.Compose([
    transforms.ToTensor(),  # 이미지를 텐서 형태로 변환
    transforms.Normalize((0.5,), (0.5,))  # 이미지를 정규화하여 평균을 0.5, 표준편차를 0.5로 조정
])

# 학습 및 테스트 데이터셋 불러오기
trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)
testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)

# 데이터로더 생성
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)
testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)

# 신경망 구조 정의
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, 5)  # 1채널의 이미지를 받아 6채널로 출력하는 5x5 컨볼루션 레이어
        self.pool = nn.MaxPool2d(2, 2)   # 2x2 윈도우로 최대 풀링을 수행하는 풀링 레이어
        self.conv2 = nn.Conv2d(6, 16, 5) # 6채널의 이미지를 받아 16채널로 출력하는 5x5 컨볼루션 레이어
        self.fc1 = nn.Linear(16 * 4 * 4, 120) # 16x4x4 크기의 텐서를 입력으로 받아 120개의 뉴런을 가지는 완전 연결 레이어
        self.fc2 = nn.Linear(120, 84)         # 120개의 입력을 받아 84개의 뉴런을 가지는 완전 연결 레이어
        self.fc3 = nn.Linear(84, 10)          # 84개의 입력을 받아 10개의 뉴런을 가지는 완전 연결 레이어

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))  # 첫 번째 컨볼루션 레이어를 거치고 풀링 레이어를 통과
        x = self.pool(F.relu(self.conv2(x)))  # 두 번째 컨볼루션 레이어를 거치고 풀링 레이어를 통과
        x = x.view(-1, 16 * 4 * 4)            # 텐서를 1차원으로 평탄화
        x = F.relu(self.fc1(x))               # 첫 번째 완전 연결 레이어를 거치고 활성화 함수를 적용
        x = F.relu(self.fc2(x))               # 두 번째 완전 연결 레이어를 거치고 활성화 함수를 적용
        x = self.fc3(x)                       # 출력 레이어를 통과
        return x

# 신경망 인스턴스 생성
net = Net()

# 훈련 함수 정의
def train(model, train_loader, test_loader, epochs=5):
    criterion = nn.CrossEntropyLoss()                        # 손실 함수 정의
    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)  # 옵티마이저 정의
    
    history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}  # 히스토리 기록을 위한 딕셔너리 생성
    
    for epoch in range(epochs):
        model.train()  # 모델을 훈련 모드로 설정
        running_loss = 0.0
        correct_train = 0
        total_train = 0
        
        for inputs, labels in train_loader:
            optimizer.zero_grad()    # 그래디언트 초기화
            outputs = model(inputs)  # 모델에 입력 전달
            loss = criterion(outputs, labels)  # 손실 계산
            loss.backward()          # 역전파
            optimizer.step()         # 옵티마이저로 모델 가중치 업데이트
            
            running_loss += loss.item()  # 손실 누적
            _, predicted = torch.max(outputs, 1)  # 예측된 클래스 확인
            total_train += labels.size(0)          # 전체 데이터 수 누적
            correct_train += (predicted == labels).sum().item()  # 올바른 예측 수 누적
        
        train_loss = running_loss / len(train_loader)  # 훈련 손실 계산
        train_acc = 100 * correct_train / total_train  # 훈련 정확도 계산
        history['train_loss'].append(train_loss)       # 훈련 손실 기록
        history['train_acc'].append(train_acc)         # 훈련 정확도 기록
        
        # 테스트 세트에 대한 성능 평가
        model.eval()  # 모델을 평가 모드로 설정
        test_loss = 0.0
        correct_test = 0
        total_test = 0
        predictions = []  # 예측 결과 저장을 위한 리스트
        
        with torch.no_grad():
            for inputs, labels in test_loader:
                outputs = model(inputs)  # 모델에 입력 전달
                loss = criterion(outputs, labels)  # 손실 계산
                test_loss += loss.item()  # 손실 누적
                _, predicted = torch.max(outputs, 1)  # 예측된 클래스 확인
                total_test += labels.size(0)  # 전체 데이터 수 누적
                correct_test += (predicted == labels).sum().item()  # 올바른 예측 수 누적
                predictions.append((inputs, predicted, labels))  # 예측 결과 저장
        
        test_loss /= len(test_loader)  # 테스트 손실 계산
        test_acc = 100 * correct_test / total_test  # 테스트 정확도 계산
        history['test_loss'].append(test_loss)        # 테스트 손실 기록
        history['test_acc'].append(test_acc)          # 테스트 정확도 기록
        
        # 에포크마다 결과 출력
        print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')
    
    return history, predictions  # 훈련 함수의 반환값은 히스토리와 예측 결과

# 신경망 훈련
hist, predictions = train(net, trainloader, testloader, epochs=5)

# 손실과 정확도 그래프로 시각화
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(hist['train_loss'], label='Train Loss', color='blue')
plt.plot(hist['test_loss'], label='Test Loss', color='red')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Test Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(hist['train_acc'], label='Train Accuracy', color='blue')
plt.plot(hist['test_acc'], label='Test Accuracy', color='red')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Test Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

# 예측된 이미지 출력
num_images = 5
num_rows = (len(predictions) + 4) // 5  # 필요한 행 수 계산

fig, axs = plt.subplots(num_rows, 5, figsize=(10, 2 * num_rows))

for i, (images, predicted, labels) in enumerate(predictions):
    row = i // 5
    col = i % 5
    for j in range(len(images)):
        img = images[j] / 2 + 0.5  # 이미지를 정규화 해제
        npimg = img.numpy()
        axs[row, col].imshow(np.transpose(npimg, (1, 2, 0)))
        axs[row, col].set_title(f'Predicted: {predicted[j]} / Actual: {labels[j]}')
        axs[row, col].axis('off')

# 빈 subplot 숨기기
for i in range(len(predictions), num_rows * 5):
    row = i // 5
    col = i % 5
    axs[row, col].axis('off')

plt.tight_layout()
plt.show()
